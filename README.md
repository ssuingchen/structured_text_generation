# structured_text_generation
# Background
During the stay-home period, I had iterated through thinking about what to eat and started to get tired of making meals. That has prompted me to create a model that will generate infinite combination of ingredients and instructions for me through one click. Recipes are comprised of a typical structure, including their titles, ingredients and instructions. Therefore, I trained one LSTM and one GPT-2 language model on the recipes with special tokens specifying their structures and generated complete recipes. Namely, the generated instructions are related to its title and ingredients. For both models, I added special tokens and generated recipes with no repeated N-grams. Moreover, the GPT-2 generated contents were also generated by considering the number of beams in order to capture hidden high probability words.

# Result
Through this project, I realized that it is possible to teach GPT-2 how to generate structured text. This was achieved by adding special control tokens to the training data. However, the fine-tuned model still needs improvement because the generated ingredients and instructions sometimes did not contain food entities named in the titles. Furthermore, I realized that pure LSTM model wasn't enough for generating structured text. The detailed code was provided in this repository.

# This repository contains:

1. A README file
2. A GPT-2 generated structured recipe csv file
3. A LSTM training code
4. A GPT-2 training code
5. A GPT-2 baseline code
